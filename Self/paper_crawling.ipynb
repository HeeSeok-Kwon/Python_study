{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paper_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33SQ3gGRGHZ1",
        "outputId": "7593b109-0a70-4183-d110-73fc1edccf37"
      },
      "source": [
        "pip install selenium"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.0.0-py3-none-any.whl (954 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 36.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 194 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 204 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 215 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 225 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 286 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 307 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 317 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 337 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 389 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 399 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 409 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 419 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 430 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 440 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 450 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 481 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 491 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 501 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 512 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 522 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 532 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 542 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 552 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 563 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 573 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 583 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 604 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 614 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 634 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 645 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 665 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 675 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 686 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 696 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 706 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 716 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 727 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 737 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 747 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 757 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 768 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 778 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 788 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 798 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 808 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 819 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 829 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 839 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 849 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 860 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 870 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 880 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 890 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 901 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 911 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 931 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 942 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 952 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 954 kB 15.5 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 82.6 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 53.7 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.5.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.20)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-35.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.0.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGKq3brGGR7b",
        "outputId": "e06b1786-cf08-4d09-d4b2-3061f01f1ea2"
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.39)] [Co\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (91.189.91.39)] [Connected to cloud.r-pro\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.39)] [Wa\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r0% [2 InRelease gpgv 242 kB] [6 InRelease 22.9 kB/74.6 kB 31%] [Connecting to s\r                                                                               \rHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,210 kB]\n",
            "Ign:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,803 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [640 kB]\n",
            "Get:17 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [69.5 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,803 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [923 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,431 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,367 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [607 kB]\n",
            "Fetched 13.1 MB in 3s (5,062 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 95.4 MB of archives.\n",
            "After this operation, 323 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 94.0.4606.71-0ubuntu0.18.04.1 [1,136 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 94.0.4606.71-0ubuntu0.18.04.1 [85.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 94.0.4606.71-0ubuntu0.18.04.1 [4,161 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 94.0.4606.71-0ubuntu0.18.04.1 [4,964 kB]\n",
            "Fetched 95.4 MB in 1s (65.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_94.0.4606.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_94.0.4606.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_94.0.4606.71-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_94.0.4606.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (94.0.4606.71-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhtr5MWGHIYE"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import csv\n",
        "import urllib # 한글 - 유니코드 인코딩\n",
        "import re\n",
        "from selenium import webdriver\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpyVlC5AHKP1"
      },
      "source": [
        "Author = []\n",
        "Title_kor = []\n",
        "Title_eng = []\n",
        "Book = []\n",
        "Keyword = []\n",
        "Txt_kor= []\n",
        "Txt_eng = []\n",
        "Url = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppJej7pvHORp"
      },
      "source": [
        "def get_URL(page, word):\n",
        "    word = urllib.parse.quote(word)\n",
        "    url_before_page=\"http://www.riss.kr/search/Search.do?isDetailSearch=N&searchGubun=true&viewYn=OP&queryText=&strQuery=\"+word+\"&exQuery=&exQueryText=&order=%2FDESC&onHanja=false&strSort=RANK&p_year1=&p_year2=&iStartCount=\"\n",
        "    url_after_page=\"&orderBy=&mat_type=&mat_subtype=&fulltext_kind=&t_gubun=&learning_type=&ccl_code=&inside_outside=&fric_yn=&image_yn=&gubun=&kdc=&ttsUseYn=&l_sub_code=&fsearchMethod=search&sflag=1&isFDetailSearch=N&pageNumber=1&resultKeyword=%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8&fsearchSort=&fsearchOrder=&limiterList=&limiterListText=&facetList=&facetListText=&fsearchDB=&icate=re_a_kor&colName=re_a_kor&pageScale=10&isTab=Y&regnm=&dorg_storage=&language=&language_code=&clickKeyword=&relationKeyword=&query=\"+word\n",
        "    url=url_before_page+page+url_after_page\n",
        "    return url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRAaGr-HPai"
      },
      "source": [
        "def get_link(search_word, page_num):\n",
        "    for i in range(page_num):\n",
        "        current_page=i*10\n",
        "        url=get_URL(str(current_page), search_word)\n",
        "        source_code_from_URL = urllib.request.urlopen(url)\n",
        "        soup=BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
        "        try: \n",
        "            page_link = soup.select(\"li > div.cont > p.title > a\") \n",
        "        except:\n",
        "            page_link = \"\"\n",
        "        \n",
        "        if i != 0 and i%2 == 0: \n",
        "            print(\"Waiting...for get URL\") \n",
        "            time.sleep(10) \n",
        "\n",
        "        print(\"crawling start page {}\".format(i))\n",
        "        for j in range(10):\n",
        "            try:\n",
        "              page_url = \"http://riss.or.kr\"+page_link[j].attrs['href'] \n",
        "            except:\n",
        "              print(\"Index error!\")\n",
        "              break\n",
        "            # print(page_url)\n",
        "            reference_data = get_reference(page_url)\n",
        "\n",
        "            Author.append(reference_data[0])\n",
        "            Title_kor.append(reference_data[1])\n",
        "            Title_eng.append(reference_data[2])\n",
        "            Book.append(reference_data[3])\n",
        "            Keyword.append(reference_data[4])\n",
        "            Txt_kor.append(reference_data[5])\n",
        "            Txt_eng.append(reference_data[6])\n",
        "            Url.append(reference_data[7])\n",
        "\n",
        "        print(\"crawling done page {}\".format(i))\n",
        "\n",
        "    mydict = {'저자': Author,\n",
        "              '국문제목': Title_kor,\n",
        "              '영문제목': Title_eng,\n",
        "              '수록지': Book,\n",
        "              '핵심어': Keyword,\n",
        "              '국문 요약':Txt_kor,\n",
        "              '영문 요약':Txt_eng,\n",
        "              '링크': Url}\n",
        "    \n",
        "    save_csv(mydict, search_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MHuHBC_HUGx"
      },
      "source": [
        "def get_reference(url):\n",
        "    # colab에서 사용할 때 옵션들\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless') \n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(\"chromedriver\", options=chrome_options) \n",
        "    driver.get(url) \n",
        "    time.sleep(2)\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    title = soup.find(\"h3\",{\"class\":\"title\"})\n",
        "    \n",
        "    title_txt = title.get_text(\"\", strip=True).split('=')\n",
        "    \n",
        "    title_kor = re.sub(\"\\n\\b\", \"\", str(title_txt[0]).strip())\n",
        "    try:\n",
        "      title_eng = str(title_txt[1]).strip()\n",
        "    except:\n",
        "      title_eng = \"None\"\n",
        "    \n",
        "    txt_box = []\n",
        "    \n",
        "    for text in soup.find_all(\"div\", {\"class\":\"text\"}):\n",
        "        txt = text.get_text(\"\", strip=True)\n",
        "        txt_box.append(txt)\n",
        "    \n",
        "    try:\n",
        "      txt_kor=txt_box[1]\n",
        "    except:\n",
        "      txt_kor = \"None\"\n",
        "    try:\n",
        "      txt_eng=txt_box[3]\n",
        "    except:\n",
        "      txt_eng = \"None\"\n",
        "    \n",
        "    author = \"None\"\n",
        "    book = \"None\"\n",
        "    keyword = \"None\"\n",
        "    detail_box = []\n",
        "    detail_info = soup.select(\"#soptionview > div > div > div.infoDetail.on > div.infoDetailL > ul > li > div > p\") \n",
        "    \n",
        "    for detail in detail_info:\n",
        "        detail_concept=detail.get_text(\"\", strip=True)\n",
        "        \n",
        "        detail_wrap=[]\n",
        "        detail_wrap.append(detail_concept)\n",
        "        \n",
        "        detail_box.append(detail_wrap)\n",
        "        \n",
        "    author = \",\".join(detail_box[0])\n",
        "    \n",
        "    book=(\n",
        "        \"\".join(detail_box[2]+detail_box[3]).replace(\"\\n\", \"\").replace(\"\\t\",\"\").replace(\" \",\"\")\n",
        "        + \"p.\"\n",
        "        +\"\".join(detail_box[-2])\n",
        "    )\n",
        "    \n",
        "    keyword=\",\".join(detail_box[6])\n",
        "        \n",
        "    reference_data = [\n",
        "        author,\n",
        "        title_kor,\n",
        "        title_eng,\n",
        "        book,\n",
        "        keyword,\n",
        "        txt_kor,\n",
        "        txt_eng,\n",
        "        url,\n",
        "    ]\n",
        "\n",
        "    driver.close()\n",
        "    return reference_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlf5H4tQHZlR"
      },
      "source": [
        "def save_csv(mydict, search_word):\n",
        "    dataframe = pd.DataFrame.from_dict(mydict, orient='index')\n",
        "    dataframe = dataframe.transpose()\n",
        "    dataframe.to_csv(search_word+'_paper_crawling.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHWKm_LFFU4L",
        "outputId": "84612d79-0c8e-477c-d425-ec08b172abe1"
      },
      "source": [
        "search_word = '블록체인'\n",
        "page_num = 10\n",
        "get_link(search_word, page_num)\n",
        "\n",
        "# install selenium : https://chancoding.tistory.com/136 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crawling start page 0\n",
            "crawling done page 0\n",
            "crawling start page 1\n",
            "crawling done page 1\n",
            "Waiting...for get URL\n",
            "crawling start page 2\n",
            "crawling done page 2\n",
            "crawling start page 3\n",
            "crawling done page 3\n",
            "Waiting...for get URL\n",
            "crawling start page 4\n",
            "crawling done page 4\n",
            "crawling start page 5\n",
            "crawling done page 5\n",
            "Waiting...for get URL\n",
            "crawling start page 6\n",
            "crawling done page 6\n",
            "crawling start page 7\n",
            "crawling done page 7\n",
            "Waiting...for get URL\n",
            "crawling start page 8\n",
            "crawling done page 8\n",
            "crawling start page 9\n",
            "crawling done page 9\n"
          ]
        }
      ]
    }
  ]
}